{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19c62e3-bdde-461e-a880-2af8d43e5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#read in reduced csv from files on local machine\n",
    "#df = pd.read_csv('nashville_reviews.csv')\n",
    "df = pd.read_csv('/Users/devpatel/Desktop/Yelp Restaurant Recommendation/nashville_reviews.csv')\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fea9b7e-9b1e-4444-9b2d-f11f1a64363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8168dd85-8a58-4c46-8e38-605e4ef5e0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0644556-782d-4f50-9135-0c14df21f4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "        Unnamed: 0                                               text  stars\n0              474  Ive been coming to this store since 2008 for a...      5\n1              475  Good selection. Friendly staff. Slightly hard ...      5\n2              476  Poor poor poor customer service. Id give 0 sta...      1\n3              477  I happened to travel across the country and st...      4\n4              478  Awful ownership at this smoke shop. I\\ve been ...      1\n...            ...                                                ...    ...\n452248     6990070  I love it here. Radnor is a short drive from t...      5\n452249     6990071  This was such a beautiful park. Me and my fian...      5\n452250     6990072  Radnor Lake State Park is has 1400 acres and i...      5\n452251     6990073  Love it!  The park is beautiful - the lake is ...      5\n452252     6990074  Another day at this beautiful park spent hike ...      5\n\n[452253 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>stars</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>474</td>\n      <td>Ive been coming to this store since 2008 for a...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>475</td>\n      <td>Good selection. Friendly staff. Slightly hard ...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>476</td>\n      <td>Poor poor poor customer service. Id give 0 sta...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>477</td>\n      <td>I happened to travel across the country and st...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>478</td>\n      <td>Awful ownership at this smoke shop. I\\ve been ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>452248</th>\n      <td>6990070</td>\n      <td>I love it here. Radnor is a short drive from t...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>452249</th>\n      <td>6990071</td>\n      <td>This was such a beautiful park. Me and my fian...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>452250</th>\n      <td>6990072</td>\n      <td>Radnor Lake State Park is has 1400 acres and i...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>452251</th>\n      <td>6990073</td>\n      <td>Love it!  The park is beautiful - the lake is ...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>452252</th>\n      <td>6990074</td>\n      <td>Another day at this beautiful park spent hike ...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>452253 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "760e29f5-78bb-45e4-bf98-eb73b5d25310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c495e50-c9c5-423c-a9ca-7baefabe6297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     text  stars\n0       Ive been coming to this store since 2008 for a...      5\n1       Good selection. Friendly staff. Slightly hard ...      5\n2       Poor poor poor customer service. Id give 0 sta...      1\n3       I happened to travel across the country and st...      4\n4       Awful ownership at this smoke shop. I\\ve been ...      1\n...                                                   ...    ...\n452248  I love it here. Radnor is a short drive from t...      5\n452249  This was such a beautiful park. Me and my fian...      5\n452250  Radnor Lake State Park is has 1400 acres and i...      5\n452251  Love it!  The park is beautiful - the lake is ...      5\n452252  Another day at this beautiful park spent hike ...      5\n\n[452253 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>stars</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ive been coming to this store since 2008 for a...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Good selection. Friendly staff. Slightly hard ...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Poor poor poor customer service. Id give 0 sta...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I happened to travel across the country and st...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Awful ownership at this smoke shop. I\\ve been ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>452248</th>\n      <td>I love it here. Radnor is a short drive from t...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>452249</th>\n      <td>This was such a beautiful park. Me and my fian...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>452250</th>\n      <td>Radnor Lake State Park is has 1400 acres and i...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>452251</th>\n      <td>Love it!  The park is beautiful - the lake is ...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>452252</th>\n      <td>Another day at this beautiful park spent hike ...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>452253 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b571b16c-05a6-448e-84ab-7f2a0b458336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('int64')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stars'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be3eb35e-a1d4-4ab3-90d0-3bd7abd78e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stars'] = df['stars'].replace({1: 0, 2: 1, 3: 2, 4: 3, 5: 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6687f42a-be0d-4145-8a2c-098e2790f222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     text  stars\n0       Ive been coming to this store since 2008 for a...      4\n1       Good selection. Friendly staff. Slightly hard ...      4\n2       Poor poor poor customer service. Id give 0 sta...      0\n3       I happened to travel across the country and st...      3\n4       Awful ownership at this smoke shop. I\\ve been ...      0\n...                                                   ...    ...\n452248  I love it here. Radnor is a short drive from t...      4\n452249  This was such a beautiful park. Me and my fian...      4\n452250  Radnor Lake State Park is has 1400 acres and i...      4\n452251  Love it!  The park is beautiful - the lake is ...      4\n452252  Another day at this beautiful park spent hike ...      4\n\n[452253 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>stars</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ive been coming to this store since 2008 for a...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Good selection. Friendly staff. Slightly hard ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Poor poor poor customer service. Id give 0 sta...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I happened to travel across the country and st...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Awful ownership at this smoke shop. I\\ve been ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>452248</th>\n      <td>I love it here. Radnor is a short drive from t...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>452249</th>\n      <td>This was such a beautiful park. Me and my fian...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>452250</th>\n      <td>Radnor Lake State Park is has 1400 acres and i...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>452251</th>\n      <td>Love it!  The park is beautiful - the lake is ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>452252</th>\n      <td>Another day at this beautiful park spent hike ...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>452253 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4779cacd-7f9f-47c3-9319-cc828b3762b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6c2c1a-059c-4a14-a581-dece73efce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'stars': 'labels'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "553ebd6a-4007-48ba-bd95-7dccd3600ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                                     text  labels\n0       Ive been coming to this store since 2008 for a...       4\n1       Good selection. Friendly staff. Slightly hard ...       4\n2       Poor poor poor customer service. Id give 0 sta...       0\n3       I happened to travel across the country and st...       3\n4       Awful ownership at this smoke shop. I\\ve been ...       0\n...                                                   ...     ...\n452248  I love it here. Radnor is a short drive from t...       4\n452249  This was such a beautiful park. Me and my fian...       4\n452250  Radnor Lake State Park is has 1400 acres and i...       4\n452251  Love it!  The park is beautiful - the lake is ...       4\n452252  Another day at this beautiful park spent hike ...       4\n\n[452253 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ive been coming to this store since 2008 for a...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Good selection. Friendly staff. Slightly hard ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Poor poor poor customer service. Id give 0 sta...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I happened to travel across the country and st...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Awful ownership at this smoke shop. I\\ve been ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>452248</th>\n      <td>I love it here. Radnor is a short drive from t...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>452249</th>\n      <td>This was such a beautiful park. Me and my fian...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>452250</th>\n      <td>Radnor Lake State Park is has 1400 acres and i...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>452251</th>\n      <td>Love it!  The park is beautiful - the lake is ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>452252</th>\n      <td>Another day at this beautiful park spent hike ...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>452253 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f79653f-8921-4bd6-8c4c-fe529a353451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['labels'], test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8977fd2b-a689-4a93-b83c-e75cbc12b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.tolist()\n",
    "val_labels = val_labels.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04ea849f-22b5-47fe-bb04-5b2496017b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b9f91aa-58a8-4aad-aebf-5ec109fc9a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361802"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02ea0def-22f0-417d-9d73-a76d4ad069bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361802"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daacbed7-b952-4e48-b813-632d94c96ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90451"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7b1840e-8509-491a-8038-6b6f1024e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_list = train_texts.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ccfe9cd-e6f8-4241-9f42-3f543aa69c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_texts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa168cf8-ecc0-46f0-97ca-5ab73337760a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8876b265-60ff-4607-9435-52e1d15641f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts_list = val_texts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b9afe-3cb6-4eaf-883e-adab48b5ccc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc748f1-c30d-4436-b09c-83abc3db94c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc46cb4d-98a0-4d69-9e83-2e8cd426d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b3fe8fb-3af6-430b-a5ee-6793727eff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts_list, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9634bc-564c-40bd-8e8f-6dbc65849455",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = train_encodings['input_ids']\n",
    "len(train_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790fc24b-269a-40b3-ba58-b02b9a69ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoding_mask = train_encodings['attention_mask'] # Assuming you're using attention_mask as well\n",
    "len(train_encoding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "041212d9-6776-4ba3-9091-0315df49d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings = tokenizer(val_texts_list, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac876655-545a-44c4-8693-e31e66278302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class YelpDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = YelpDataset(train_encodings, train_labels)\n",
    "val_dataset = YelpDataset(val_encodings, val_labels)\n",
    "#test_dataset = YelpDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b872f88-2b99-4dc0-b45c-e4a38f922b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463ab12-6e1e-4d23-92fa-aa34db993ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a0427-6cab-4310-8670-1da18e4b7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552203bf-7c96-4a50-9591-5158354cef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932923d-f025-43ba-904f-2e315ac40b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc06c9b6-d143-43fc-9aeb-57f497585594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 16:18:44.703139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d8f21a288444a53bc26959bc8cb9264"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/devpatel/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     18\u001B[0m labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 19\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     21\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:763\u001B[0m, in \u001B[0;36mDistilBertForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    756\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m    757\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m    758\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m    759\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m    760\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    761\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m--> 763\u001B[0m distilbert_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistilbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    764\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    766\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    767\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    768\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    771\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    772\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m distilbert_output[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, seq_len, dim)\u001B[39;00m\n\u001B[1;32m    773\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m hidden_state[:, \u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, dim)\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:583\u001B[0m, in \u001B[0;36mDistilBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    579\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    581\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(input_ids, inputs_embeds)  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[0;32m--> 583\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    584\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    585\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    586\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    587\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    588\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    589\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:359\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[1;32m    357\u001B[0m     all_hidden_states \u001B[38;5;241m=\u001B[39m all_hidden_states \u001B[38;5;241m+\u001B[39m (hidden_state,)\n\u001B[0;32m--> 359\u001B[0m layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    362\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    364\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:295\u001B[0m, in \u001B[0;36mTransformerBlock.forward\u001B[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001B[0m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;124;03mParameters:\u001B[39;00m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001B[39;00m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;66;03m# Self-Attention\u001B[39;00m\n\u001B[0;32m--> 295\u001B[0m sa_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    299\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    300\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    301\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    302\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n\u001B[1;32m    304\u001B[0m     sa_output, sa_weights \u001B[38;5;241m=\u001B[39m sa_output  \u001B[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:226\u001B[0m, in \u001B[0;36mMultiHeadSelfAttention.forward\u001B[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001B[0m\n\u001B[1;32m    221\u001B[0m mask \u001B[38;5;241m=\u001B[39m (mask \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mview(mask_reshp)\u001B[38;5;241m.\u001B[39mexpand_as(scores)  \u001B[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001B[39;00m\n\u001B[1;32m    222\u001B[0m scores \u001B[38;5;241m=\u001B[39m scores\u001B[38;5;241m.\u001B[39mmasked_fill(\n\u001B[1;32m    223\u001B[0m     mask, torch\u001B[38;5;241m.\u001B[39mtensor(torch\u001B[38;5;241m.\u001B[39mfinfo(scores\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39mmin)\n\u001B[1;32m    224\u001B[0m )  \u001B[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001B[39;00m\n\u001B[0;32m--> 226\u001B[0m weights \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001B[39;00m\n\u001B[1;32m    227\u001B[0m weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(weights)  \u001B[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001B[39;00m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;66;03m# Mask heads if we want to\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1841\u001B[0m, in \u001B[0;36msoftmax\u001B[0;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[1;32m   1839\u001B[0m     dim \u001B[38;5;241m=\u001B[39m _get_softmax_dim(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim(), _stacklevel)\n\u001B[1;32m   1840\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1841\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1842\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1843\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msoftmax(dim, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(\"batch complete\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c0c7d-48fc-4bd4-8a8d-d3dd97e77973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4dd019-796a-4c13-b3f4-5b977a43f2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
